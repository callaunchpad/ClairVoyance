{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Beauty and Joy of Tensorflow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This tutorial will give you a low level look into how tensorflow operates so you feel comfortable with the API\n",
    "and run you through a basic NN tutorial. From there you should be able to pick up future models and use them with\n",
    "tensorflow functions. This is also assuming you understand basic NN architecture and optimization\n",
    "(you can check the education doc to review if needed).\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we need to import tensorflow and helper libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Sweet! Now what? Well first let's start by \n",
    "importing the mother of all ML data: MNIST. We will use for our example for a basic NN.\n",
    "We will import the dataset from keras and load in the training the testing separately\"\"\"\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "#Here Keras automatically splits our dataset into training and testing along with inputs and labels. Let's see what they look like\n",
    "\"\"\"(x_train, y_train), (x_test, y_test) tf.keras.datasets.cifar10.load_data()\n",
    "In case you want to try CIFAR (NOTE: It would be good to check shape of Cifar and properly adjust since it is )\n",
    "\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see what the shape of our data looks like\n",
    "print(\"shape of our training input: {}\".format(x_train.shape))\n",
    "print(\"shape of our training labels: {}\".format(y_train.shape))\n",
    "print(x_train[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Find a way to check which number the image x_train[0] represents without seeing the image itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"So we have 60,000 training examples (which is a lot!). But I want to see the digits themselves!\n",
    "This is where we can use some matplotlib, which will be useful when we want to check say our weather images\n",
    "predictions\"\"\"\n",
    "plt.imshow(x_train[3])\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Looks like a 1! There are other ways to display images with PIL and Scipy \n",
    "but MatPlotLib tends to have more features. Ok but something to notice is that the labels are normal digits.\n",
    "Usually when have a neural model for classification, it will output a vector of probabilities for each category\n",
    "EX with 5 categories: [0.01, 0.12, 0.84, 0.0, 0.03]\n",
    "Because of this, we want our labels themselves to be a vector of probabilites to train the model with.\n",
    "EX if label is 2: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] since ideally we want the 100% probability for category 2 and 0 elsewhere\n",
    "In order to do this we do an operation called one_hot.\"\"\"\n",
    "\n",
    "\n",
    "#NOTE: I originally had this as a to-do where you use tf.one_hot. Unfortunately that function returns a tensor instead of a\n",
    "#numpy matrix which we cannot input into our model. It's still usable in roundabout ways but for the purposes of this\n",
    "#tutorial, I just one-hoted the labels for you. It would be good to look at what tf.one_hot does though.\n",
    "numpy_zero = np.zeros((y_train.shape[0], 10))\n",
    "numpy_zero[np.arange(y_train.shape[0]), y_train] = 1\n",
    "y_train = numpy_zero\n",
    "numpy_zero2 = np.zeros((y_test.shape[0], 10))\n",
    "numpy_zero2[np.arange(y_test.shape[0]), y_test] = 1\n",
    "y_test = numpy_zero2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "'Ok Pardhu this is great and all but I came here to learn tensorflow, not look at handwritten digits'\n",
    "Alright well I won't hold you off any longer! Let's get into what a tensorflow model will look like.\n",
    "While Arsh will have a more complete workshop on tensorflow, there are two important stages to building\n",
    "a model on tensorflow.\n",
    "1. Define the abstract graph of the model along with all important variables inside the graph and how it will train.\n",
    "2. Open up a tensorflow session to then feed in the inputs and train over epochs\n",
    "Let's start with part one and see how that looks:\n",
    "\"\"\"\n",
    "#Define our model through a function:\n",
    "def ourModel(inputs):\n",
    "    #We assume the shape of inputs is [None, 784]. None represents an undefined, variable size for our number of images. \n",
    "    #But why 784?.\n",
    "    #Let's create our first neural layer that creates a [None, 128] matrix from the inputs matrix.\n",
    "    first_layer = tf.layers.dense(inputs=inputs, units=128)\n",
    "    \n",
    "    #But wait! The whole point of neural networks is our activation function. Let's use relu in the next line.\n",
    "    #If confused why, check out https://www.analyticsvidhya.com/blog/2017/10/fundamentals-deep-learning-activation-functions-when-to-use-them/\n",
    "    first_layer = tf.nn.relu(first_layer)\n",
    "    \n",
    "    #And that's it! Though it seems a bit unclean to have separate lines like that. Let's simplify for the next layer\n",
    "    second_layer = tf.layers.dense(inputs=first_layer, units=64, activation=tf.nn.relu)\n",
    "    \"\"\"Sweet! We have two layers that each have relu activation functions.\n",
    "    TODO: Now it is up to you to finish the model. Add one more hidden layer with 32 units and then the output layer\"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"NOTE: We want our output layer to be of size [None, 10] since there are 10 digits and we can use the vector as\n",
    "    a probability output. But we want the probability to be between 0 to 1 which relu doesn't do. What other activation\n",
    "    funciton can we use? HINT: Look at the above article\"\"\"\n",
    "    return output_layer\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here we have defined the operation of our model through a function. But this isn't actually where we create\n",
    "the tensorflow graph. We can do that right below like a normal python script. The important thing is that this\n",
    "is recognized as a graph model for tf because our operations are done on what we define as tf placeholders\n",
    "When we actually run our session. We will feed in numpy matrices into the place holders.\n",
    "It is important to recognize that our placeholders have defined shapes. Tensorflow will automatically compute\n",
    "the shapes at each level of the model based on this, essentially making it a \"fixed\" graph\n",
    "\"\"\"\n",
    "inputs=tf.placeholder(tf.float32, shape=[None, 784])\n",
    "#Because normal dense layers take in 1D data to connect to each neuron, we can't feed in the image as a 2D matrix.\n",
    "#Instead for each image we will feed in each pixel: 28x28 = 784. Did you catch that when we built the model?\n",
    "\n",
    "labels = 0 #TODO: Make a placeholder for the labels which will be a vector of size 10 for each image.\n",
    "\n",
    "#Now we can create the abstract graph by essentially using these placeholders. Nothing actually runs in the following lines.\n",
    "#All that happens is tf will use the placeholder shapes to then create space based on the defined model, so that each\n",
    "#step has defined tensor shapes\n",
    "outputs = ourModel(inputs)\n",
    "\n",
    "#That was easy! Just one line in this case! Outputs stores predictions based on the given inputs\n",
    "\n",
    "#Ok now let's define our loss and how we will train it. It's ok if you don't understand these specific loss functions\n",
    "#and optimizers. You can look into these in your own time but for this notebook, you can consider these standard for classification\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=outputs, labels=labels))\n",
    "\n",
    "#And now we have to optimize this loss\n",
    "trainer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)\n",
    "\n",
    "#Let's also define accuracy for our viewing\n",
    "_, accuracy = tf.metrics.accuracy(tf.argmax(labels, 1), tf.argmax(outputs, 1))\n",
    "\n",
    "#Let's set some hyperparamaters for batch size and epochs\n",
    "batch_size = 128\n",
    "num_epochs = 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NOTE: You might have gotten some warning from the previous cell. You can just ignore that :)\n",
    "We have constructed our model! Now let's move on to actually training it. This is where we have to run\n",
    "a tensorflow session in the following way\n",
    "\"\"\"\n",
    "#First let's scale the values of our images from [0, 255] (as we saw earlier) to [0, 1]\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0 \n",
    "x_train = 0 #TODO: keep reading until sess.run then uncomment and finish this. There are many ways of doing this task but look into a reshape operation in numpy\n",
    "x_test = 0\n",
    "\n",
    "init=tf.global_variables_initializer() #initializer of all variables (The random weights of the model before training).\n",
    "local_init = tf.local_variables_initializer() #This isn't always needed but tf.metrics.accuracy has local variables to initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #each call to sess.run() runs the approriate input within our tf session\n",
    "    sess.run(init)\n",
    "    sess.run(local_init)\n",
    "    print(\"Starting to run session...\")\n",
    "    for epoch in range(num_epochs): #iterate through each epoch\n",
    "        for i in range(x_train.shape[0]//batch_size): #iterate through the number of batches we can make from our samples\n",
    "            batch_images = x_train[i*batch_size:(i+1)*batch_size]\n",
    "            batch_labels = y_train[i*batch_size:(i+1)*batch_size]\n",
    "            #We have a batch of training images and labels. \n",
    "            #Now we need to run the model by inputing these into the place holders we made\n",
    "            _ = sess.run([trainer], feed_dict={inputs: batch_images, labels: batch_labels})\n",
    "            \"\"\"The output of sess.run will be the input array. Essentially, the model will run\n",
    "            up until that variable by feeding in the corresponding placeholders in feed_dict.\n",
    "            EX: model_loss = sess.run([loss], feed_dict={inputs: batch_images, labels: batch_labels}) will run\n",
    "            until the loss variable of the model is calculated, stop, and return it.\n",
    "            \n",
    "            Your return array can have multiple variables: model_loss, _ = sess.run([loss, trainer], feed_dict={inputs: batch_images, labels: batch_labels})\n",
    "            \n",
    "            By having trainer has an output variable, we ensure that the model is run until the trainer which means\n",
    "            the model will work to minimize loss\n",
    "            \n",
    "            TODO: The sess.run line will error out. Why? Because we defined inputs as a placeholder with shape [None, 784]\n",
    "            But we are feeding in something of shape [None, 28, 28]. How can we fix this? (Look at the above TODO now).\"\"\"\n",
    "            \n",
    "        \"\"\"Once you have that fixed, we are done! Our model will train on its own\n",
    "        TODO: Ok but I want to see how well the model is doing. Here, for each epoch, let's feed in testing images\n",
    "        and output the accuracy as we defined earlier in the model. For each epoch test a different segment \n",
    "        of test data (similar to what we did in the batching).\n",
    "        NOTE: If you run into an error where your accuracy is outputting something involving <Tensor...>, \n",
    "        make sure the variable you are using to store your accuracy is NOT 'accuracy' like we defined earlier. Tensorflow, unfortunately,\n",
    "        will not store the variables separately and will confused 'accuracy' with the one defined in the graph\"\"\"\n",
    "        #YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
